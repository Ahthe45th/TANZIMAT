#!/usr/bin/env python3
"""Monitor YouTube channels and auto-download new videos.

Reads channel identifiers from ``channels.txt``. Each day, checks for
videos uploaded in the last 24 hours and downloads them using ``yt-dlp``
in 360p. Previously downloaded videos are skipped using a local cache
file. Channel identifiers may be the channel ID (``UC...``), a legacy
username, or a handle prefixed with ``@``.
"""

import datetime as _dt
import re as _re
import subprocess as _sp
import xml.etree.ElementTree as _ET
from pathlib import Path
from urllib.request import urlopen, Request
from urllib.error import HTTPError
import sys
import logging
from dotenv import load_dotenv

SCRIPT_DIR = Path(__file__).resolve().parent
CHANNELS_FILE = SCRIPT_DIR / "channels.txt"
CACHE_FILE = SCRIPT_DIR / "downloaded_videos.txt"
DOWNLOAD_DIR = Path.home() / "yt-watchlist"
LOG_FILE = SCRIPT_DIR / "youtube_watchlist.log"
TANZIMAT_ENV_FILE = SCRIPT_DIR / "tanzimat.env"

# Configure logging
logging.basicConfig(
    filename=LOG_FILE,
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# Load environment variables
if TANZIMAT_ENV_FILE.is_file():
    load_dotenv(dotenv_path=TANZIMAT_ENV_FILE)
    logging.info(f"Loaded environment variables from {TANZIMAT_ENV_FILE}")
else:
    logging.info(f"Environment file not found at {TANZIMAT_ENV_FILE}, skipping.")

def _read_channels(path: Path) -> list[str]:
    if not path.exists():
        logging.error(f"Channel list not found: {path}")
        return []
    with path.open() as f:
        return [ln.strip() for ln in f if ln.strip() and not ln.startswith("#")]


def _load_cache(path: Path) -> set[str]:
    if not path.exists():
        return set()
    with path.open() as f:
        return {ln.strip() for ln in f if ln.strip()}


def _append_cache(path: Path, video_id: str) -> None:
    with path.open("a") as f:
        f.write(video_id + "\n")


def _resolve_handle(handle: str) -> str:
    """Return the channel ID for a YouTube handle."""
    url = f"https://www.youtube.com/@{handle}"
    req = Request(url, headers={"User-Agent": "Mozilla/5.0"})
    with urlopen(req) as resp:
        html = resp.read().decode("utf-8", errors="ignore")
    match = _re.search(r'"channelId":"(UC[^"]+)"', html)
    if not match:
        raise ValueError(f"Could not resolve handle @{handle}")
    return match.group(1)


def _fetch_feed(channel: str) -> bytes:
    if channel.startswith("UC"):
        url = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel}"
        with urlopen(url) as resp:
            return resp.read()

    if channel.startswith("@"):
        try:
            channel_id = _resolve_handle(channel[1:])
        except Exception as exc:
            raise HTTPError(None, 404, str(exc), None, None)
        url = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"
        with urlopen(url) as resp:
            return resp.read()

    url = f"https://www.youtube.com/feeds/videos.xml?user={channel}"
    with urlopen(url) as resp:
        return resp.read()


def _parse_feed(xml_data: bytes) -> list[tuple[str, str]]:
    root = _ET.fromstring(xml_data)
    ns = {
        "yt": "http://www.youtube.com/xml/schemas/2015",
        "atom": "http://www.w3.org/2005/Atom",
    }
    results = []
    for entry in root.findall("atom:entry", ns):
        vid = entry.findtext("yt:videoId", namespaces=ns)
        published = entry.findtext("atom:published", namespaces=ns)
        if vid and published:
            results.append((vid, published))
    return results


def _is_recent(published: str, *, hours: int = 72) -> bool:
    try:
        ts = _dt.datetime.fromisoformat(published.rstrip("Z"))
    except ValueError:
        return False
    return _dt.datetime.now(_dt.UTC) - ts < _dt.timedelta(hours=hours)


def _get_video_duration(file_path: Path) -> float:
    """Return the duration of a video in seconds."""
    cmd = [
        "ffprobe",
        "-v",
        "error",
        "-show_entries",
        "format=duration",
        "-of",
        "default=noprint_wrappers=1:nokey=1",
        str(file_path),
    ]
    try:
        result = _sp.run(cmd, capture_output=True, text=True, check=True)
        return float(result.stdout)
    except (_sp.CalledProcessError, ValueError) as e:
        logging.error(f"Error getting duration for {file_path}: {e}")
        return 0.0


def _download(video_id: str) -> Path | None:
    DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)
    url = f"https://www.youtube.com/watch?v={video_id}"
    out_tpl = DOWNLOAD_DIR / "%(title)s [%(id)s].%(ext)s"
    cmd = ["yt-dlp", "-f", "18", url, "-o", str(out_tpl)]
    logging.info(f"Attempting to download video: {video_id}")
    proc = _sp.run(cmd, check=False, capture_output=True, text=True)
    if proc.returncode != 0:
        logging.error(f"yt-dlp failed for {video_id}: {proc.stderr}")
        return None

    # Find the downloaded file
    for line in proc.stdout.splitlines():
        if "Destination:" in line:
            filepath_str = line.split("Destination:", 1)[1].strip()
            return Path(filepath_str)
        if "has already been downloaded" in line:
            match = _re.search(r"Destination: (.+)", proc.stdout)
            if match:
                return Path(match.group(1))
    return None


def main() -> None:
    logging.info("Script started.")
    _sp.run(["notify-send", "YouTube Watchlist", "Scraping started..."], check=False)
    channels = _read_channels(CHANNELS_FILE)
    if not channels:
        logging.info("No channels found. Exiting.")
        _sp.run(["notify-send", "YouTube Watchlist", "Finished: No channels found."], check=False)
        return
    cache = _load_cache(CACHE_FILE)
    for ch in channels:
        logging.info(f"Processing channel: {ch}")
        try:
            feed = _fetch_feed(ch)
        except Exception as exc:  # network errors
            logging.error(f"Failed fetching feed for {ch}: {exc}")
            continue
        for vid, published in _parse_feed(feed):
            if vid in cache or not _is_recent(published):
                continue
            downloaded_file = _download(vid)
            if downloaded_file:
                duration = _get_video_duration(downloaded_file)
                if duration > 60:
                    logging.info(
                        f"Video {vid} is longer than 1 minute ({duration}s), deleting."
                    )
                    downloaded_file.unlink()
                else:
                    _append_cache(CACHE_FILE, vid)
                    cache.add(vid)
    logging.info("Script finished.")
    _sp.run(["notify-send", "YouTube Watchlist", "Scraping finished."], check=False)


if __name__ == "__main__":
    main()
